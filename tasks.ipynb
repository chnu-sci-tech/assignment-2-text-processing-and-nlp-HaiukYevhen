{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks for laboratory assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports section\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import gensim as gs\n",
    "from gensim.test.utils import lee_corpus_list\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "import gensim.downloader as api\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "\n",
    "# Download the necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from the json file \n",
    "\n",
    "Write a script, that would extract all the the data from the json file and return it in a list of texts for further use in the next tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extract the text data from the json file\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Relative location of the json file in the project.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of texts as strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return data['texts']\n",
    "    return None \n",
    "\n",
    "texts = extract_texts_from_file('resources/data.json')\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the texts\n",
    "\n",
    "Write a script, that would process these texts. Processing should include:\n",
    "\n",
    "- Lowercasing\n",
    "- Removing Punctuation\n",
    "- Removing Special Characters and Numbers\n",
    "- Stemming or Lemmatization\n",
    "- Handling Abbreviations\n",
    "- Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(texts):\n",
    "    \"\"\"\n",
    "    Process the texts\n",
    "\n",
    "    Args:\n",
    "        texts (list): list of str texts to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of texts as strings processed by script.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token.isalpha() and not token.isdigit()]\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        processed_texts.append(' '.join(tokens))\n",
    "    return processed_texts \n",
    "\n",
    "texts_processed = process_texts(texts)\n",
    "print(texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize as cloud of words\n",
    "\n",
    "Visualize the texts as a cloud of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_texts(texts_processed):\n",
    "    \"\"\"\n",
    "    Visualize the texts as cloud of words\n",
    "\n",
    "    Args:\n",
    "        texts_processed (list): list of texts, processed in previous task.\n",
    "\n",
    "    Returns:\n",
    "        None: None.\n",
    "    \"\"\"\n",
    "    all_text = ' '.join(texts_processed)\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(all_text)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "visualize_texts(texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarities using TFIDF\n",
    "\n",
    "Calculate the TFIDF score and cosine similarity between the texts. You may use gensim, write your own script using numpy or use other module. Output the result as a matrix $n\\times n$ of cosine similarity scores (where $n$ is the length of texts list).\n",
    "\n",
    "**Extra credit**: use word2vec word embeddings in vector spaces to get better TFIDF scores by representing each word as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(matrix):\n",
    "    \"\"\"Calculate cosine similarity matrix using numpy.\"\"\"\n",
    "    norms = np.linalg.norm(matrix, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  \n",
    "    normalized_matrix = matrix / norms\n",
    "    \n",
    "    cosine_matrix = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "    return cosine_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_tfidf(texts_processed):\n",
    "    \"\"\"\n",
    "    Calculate TFIDF score between the texts in the list.\n",
    "\n",
    "    Args:\n",
    "        texts_processed (list): list of texts, processed in previous task.\n",
    "\n",
    "    Returns:\n",
    "        cosine_matrix (list): a matrix of cosine similarity scores.\n",
    "    \"\"\"\n",
    "    tokenized_texts = [text.split() for text in texts_processed]\n",
    "    \n",
    "    dictionary = Dictionary(tokenized_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "    tfidf_model = TfidfModel(corpus)\n",
    "    tfidf_corpus = tfidf_model[corpus]\n",
    "    \n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=100,  \n",
    "        window=5,  \n",
    "        min_count=1, \n",
    "        workers=4, \n",
    "        sg=0  \n",
    "    )\n",
    "    \n",
    "    document_vectors = []\n",
    "    \n",
    "    for _, doc_bow in enumerate(tfidf_corpus):\n",
    "        doc_vector = np.zeros(word2vec_model.wv.vector_size)\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for word_id, tfidf_score in doc_bow:\n",
    "            word = dictionary[word_id]\n",
    "            \n",
    "            if word in word2vec_model.wv:\n",
    "                word_embedding = word2vec_model.wv[word]\n",
    "                doc_vector += word_embedding * tfidf_score\n",
    "                total_weight += tfidf_score\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            doc_vector = doc_vector / total_weight\n",
    "        \n",
    "        document_vectors.append(doc_vector)\n",
    "    \n",
    "    document_vectors_matrix = np.array(document_vectors)\n",
    "    \n",
    "    cosine_matrix = cosine_similarity_matrix(document_vectors_matrix)\n",
    "    \n",
    "    return cosine_matrix\n",
    "\n",
    "cosine_matrix = calcuate_tfidf(texts_processed)\n",
    "print(cosine_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data\n",
    "\n",
    "Visualize the `cosine_matrix` data using matplotlib. Choose your own method of plotting the scores in a way, that the similarity indeces between texts would be instantly visible. Plot data about texts as well (a couple of first words in the document).\n",
    "\n",
    "*Hint: remember heat maps, aren't they nice for this task?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(cosine_matrix, texts_processed):\n",
    "    \"\"\"\n",
    "    Visualize the cosine similarity matrix from the previous task. \n",
    "\n",
    "    Args:\n",
    "        cosine_matrix (list): matrix of cosine similarity scores (values from 0.0 to 1.0 expected).\n",
    "        texts_processed (list): list of texts, processed in previous task.\n",
    "\n",
    "    Returns:\n",
    "        None: None.\n",
    "    \"\"\"\n",
    "    cosine_matrix = np.array(cosine_matrix)\n",
    "    cosine_matrix = np.clip(cosine_matrix, -1, 1)\n",
    "    \n",
    "    labels = []\n",
    "    for text in texts_processed:\n",
    "        words = text.split()[:5]  \n",
    "        label = ' '.join(words)\n",
    "        if len(label) > 40:  \n",
    "            label = label[:37] + '...'\n",
    "        labels.append(label)\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    im = ax.imshow(cosine_matrix, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(labels, fontsize=9)\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Cosine Similarity', rotation=270, labelpad=20, fontsize=12)\n",
    "    \n",
    "    threshold = 0.1  \n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            value = cosine_matrix[i, j]\n",
    "            if abs(value) >= threshold or i == j: \n",
    "                text_color = \"white\" if abs(value) > 0.5 else \"black\"\n",
    "                text = ax.text(j, i, f'{value:.2f}',\n",
    "                              ha=\"center\", va=\"center\",\n",
    "                              color=text_color,\n",
    "                              fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(\"Cosine Similarity Matrix between Texts\", \n",
    "                pad=20, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(labels)) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(labels)) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "visualize_data(cosine_matrix, texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse your own text using TFIDF (or any other method)\n",
    "\n",
    "Analyse your own text using previous methods. You may use API fetching to get text data, download texts from Kaggle or use any texts you want. Find cosine similarities and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts():\n",
    "    \"\"\"\n",
    "    Get the texts you want to analyse. Either API fetching, Kaggle or any other source.\n",
    "\n",
    "    Args:\n",
    "        None: None.\n",
    "\n",
    "    Returns:\n",
    "        list: list of texts.\n",
    "    \"\"\"\n",
    "    texts = [\n",
    "        \"Lord of the Mystery best anime?\",\n",
    "        \"Best game of the year Expedition 33?\",\n",
    "        \"Haiuk Yevhen hardcore gamer\",\n",
    "        \"Twitch is dying?\",\n",
    "    ]\n",
    "    return texts\n",
    "\n",
    "texts = get_texts()\n",
    "texts_processed = process_texts(texts)\n",
    "visualize_texts(texts_processed)\n",
    "cosine_matrix = calcuate_tfidf(texts_processed)\n",
    "visualize_data(cosine_matrix, texts_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
